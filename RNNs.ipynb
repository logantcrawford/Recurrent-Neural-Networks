{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "Recurrent Neural Networks (RNNs) are a popular model for many NLP tasks as they perform best with sequential data. For this short analysis, we will use a pre-canned IMDB dataset and test the capabilities of a standard RNN, a Gated Recurrent Unit (GRU), a standard Long Short-Term Memory (LSTM), and a bidirectional LSTM. We will then look into the advantages and disadvantages of the architectures. \n",
    "\n",
    "## What's an RNN?\n",
    "RNNs are recurrent, meaning they perform the same task for every element sequence, with the output being dependent on the previous computations. This works similarly to having a memory that logs information. In theory, this architecture can handle arbitrarily long sequences; in practice, the standard RNN suffers from vanishing and exploding gradients. \n",
    "\n",
    "The vanishing gradient problem comes from the RNN function:\n",
    "\n",
    "$h_t=f(\\theta_{hh}h_{t-1}+\\theta_{xh}X_t)$\n",
    "\n",
    "therfore:\n",
    "\n",
    "$\\frac{\\partial h_t}{\\partial h_{t-1}} = f(\\theta_{hh},\\theta_{xh},h_{t-1},x_t)\\theta_{hh}$\n",
    "\n",
    "Where if $\\theta_{hh}$ is small (if the largest eigenvalue of $\\theta_{hh}$ is less than 1), then the gradient gets vanishingly tiny if there are many successive multiplications. Similarly, this is true for the largest eigenvalue. If it is greater than 1, we get exploding gradients.\n",
    "\n",
    "The problem with the vanishing gradient is that the RNN becomes incapable of capturing long-term dependencies. The exploding gradient can quickly cause even more problems; potentially, a bad parameter configuration or stochastic gradient descent (SGD) update can become too big, leading to instability in model training. Vanishing gradient problem does not always occur but could contribute to the lower validation accuracy, compared to the others that are preventative of this issue (but it still can occur).\n",
    "\n",
    "There are ways to fix the vanishing gradient problem; we use ReLU activation functions or change the RNN architecture. For this analysis, we will be changing the architecture and trying to understand why there are differences. \n",
    "\n",
    "Let's break down these architectures:\n",
    "\n",
    "## Long Short-term Memory (LSTM): \n",
    "LSTM is a Gated Recurrent Network (GRN) that alleviates some difficulties in training the RNN and deals with the vanishing gradient problem. \n",
    "\n",
    "## Gated Recurrent Units (GRU): \n",
    "GRU has some advantages over LSTM. GRU has fewer parameters by being a simpler alternative, making it faster to train and perform comparable performance. \n",
    "\n",
    "## Bidirectional Long Short-term Memory (BiLSTM):\n",
    "Two hidden states created from both LSTMs are concatenated. This gives the NN more representational power by looking into the past and future contexts by looking at both left and the right context for the current prediction. \n",
    "\n",
    "## Begining the Analysis \n",
    "We will start by importing PyTorch and Pickle, setting PyTorch parameters, and loading the pickle files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\logan\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\logan\\anaconda3\\lib\\site-packages (from torch) (3.10.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewVocabVectors = pickle.load(open('assets/reviewVocabVectors', 'rb'))\n",
    "trainIterator = pickle.load(open('assets/trainIterator', 'rb'))\n",
    "testIterator = pickle.load(open('assets/testIterator', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Architecture\n",
    "This class instantiates the four different RNN architectures with the set data to ensure consistency. The architectures can be instantiated like this `MyRNN(model='RNN')` with `RNN` being the dummy variable to generate the specific RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 100\n",
    "hiddenSize = 10\n",
    "dropoutRate = 0.5\n",
    "numEpochs = 5\n",
    "vocabSize = 20002\n",
    "pad = 1\n",
    "unk = 0\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.LSTM = (model == 'LSTM' or model == 'BiLSTM')\n",
    "        self.bidir = (model == 'BiLSTM')\n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize, padding_idx = pad)\n",
    "        \n",
    "        if model == 'RNN': \n",
    "            self.rnn = nn.RNN(embeddingSize, hiddenSize)\n",
    "        elif model == 'GRU': \n",
    "            self.rnn = nn.GRU(embeddingSize, hiddenSize)\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(embeddingSize, hiddenSize, bidirectional=self.bidir)\n",
    "\n",
    "        self.dense = nn.Linear(hiddenSize * (2 if self.bidir else 1), 1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "    def forward(self, text, textLengths):\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        packedEmbedded = nn.utils.rnn.pack_padded_sequence(embedded, textLengths)\n",
    "        \n",
    "        if self.LSTM: \n",
    "            packedOutput, (hidden, cell) = self.rnn(packedEmbedded)\n",
    "        else: \n",
    "            packedOutput, hidden = self.rnn(packedEmbedded)\n",
    "\n",
    "        output, outputLengths = nn.utils.rnn.pad_packed_sequence(packedOutput)\n",
    "        \n",
    "        if self.bidir: \n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else: \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        return self.dense(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicRNN = MyRNN(model='RNN')\n",
    "GRU = MyRNN(model='GRU') \n",
    "LSTM = MyRNN(model='LSTM') \n",
    "biLSTM = MyRNN(model='BiLSTM') \n",
    "\n",
    "models = [basicRNN, GRU, LSTM, biLSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "    model.embed.weight.data.copy_(reviewVocabVectors)\n",
    "    model.embed.weight.data[unk] = torch.zeros(embeddingSize)\n",
    "    model.embed.weight.data[pad] = torch.zeros(embeddingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def batchAccuracy(preds, targets):\n",
    "    roundedPreds = (preds >= 0)\n",
    "    return (roundedPreds == targets).sum().item() / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Architectures\n",
    "This simple loop sets hyperparameters for training and, per batch, calculates the loss from the `trainIterator` file that we previously loaded. **Be careful running this loop as it takes at least an hour to run and yields different but similar results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Epoch: 1, Train Loss: 0.6541614969977942\n",
      "Model: RNN, Epoch: 2, Train Loss: 0.6211055834275072\n",
      "Model: RNN, Epoch: 3, Train Loss: 0.5965952027941603\n",
      "Model: RNN, Epoch: 4, Train Loss: 0.582118474918863\n",
      "Model: RNN, Epoch: 5, Train Loss: 0.5689613635430251\n",
      "\n",
      "Model: GRU, Epoch: 1, Train Loss: 0.6986629676331034\n",
      "Model: GRU, Epoch: 2, Train Loss: 0.6842351510091815\n",
      "Model: GRU, Epoch: 3, Train Loss: 0.6193866354730123\n",
      "Model: GRU, Epoch: 4, Train Loss: 0.4861523292558577\n",
      "Model: GRU, Epoch: 5, Train Loss: 0.3978971023007732\n",
      "\n",
      "Model: LSTM, Epoch: 1, Train Loss: 0.6938529293555433\n",
      "Model: LSTM, Epoch: 2, Train Loss: 0.6422763431773466\n",
      "Model: LSTM, Epoch: 3, Train Loss: 0.5530681530837818\n",
      "Model: LSTM, Epoch: 4, Train Loss: 0.47816736893275813\n",
      "Model: LSTM, Epoch: 5, Train Loss: 0.39673527099592304\n",
      "\n",
      "Model: BiLSTM, Epoch: 1, Train Loss: 0.6928483648678226\n",
      "Model: BiLSTM, Epoch: 2, Train Loss: 0.6791311325624471\n",
      "Model: BiLSTM, Epoch: 3, Train Loss: 0.5798324180381073\n",
      "Model: BiLSTM, Epoch: 4, Train Loss: 0.4713036771625509\n",
      "Model: BiLSTM, Epoch: 5, Train Loss: 0.38633838406456705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.train()\n",
    "\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "        \n",
    "    torch.manual_seed(0)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(numEpochs):\n",
    "        epochLoss = 0\n",
    "        for batch in trainIterator:\n",
    "            optimizer.zero_grad()\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "        print(f'Model: {model.name}, Epoch: {epoch + 1}, Train Loss: {epochLoss / len(trainIterator)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Architectures\n",
    "Similar to training, this loop evaluates the accuracy of the predictions based on `testIterator`. This will give us a general idea of the performance of each of the architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Validation Accuracy: 75.37164322250639%\n",
      "Model: GRU, Validation Accuracy: 82.48641304347827%\n",
      "Model: LSTM, Validation Accuracy: 83.8914641943734%\n",
      "Model: BiLSTM, Validation Accuracy: 80.27733375959079%\n"
     ]
    }
   ],
   "source": [
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        if model is None:\n",
    "            continue\n",
    "        accuracy = 0.0\n",
    "        for batch in testIterator:\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = batchAccuracy(predictions, batch[1])\n",
    "            accuracy += acc\n",
    "        print('Model: {}, Validation Accuracy: {}%'.format(model.name, accuracy / len(testIterator) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Results\n",
    "Seeing accuracy for each of the trained models, we see expected results for the most part. The reason for this is the different representational power and implementation for dealing with the vanishing gradient problem.\n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "Starting with the least accurate, Recurrent Neural Networks (RNN). The Neural Network (NN) maintains a hidden \"internal\" state that applies a simple recurrence relation updated as a sequence is processed. A prevalent issue with this type of NN is the vanishing gradient problem, where the gradient signal from faraway gets lost as it becomes much smaller in magnitude than nearby signals. If too large, the stochastic gradient descent (SGD) updates can become too large. This causes instability in the model and the possibility of reaching a bad parameter.\n",
    "\n",
    "Vanishing gradient problem does not always occur but could contribute to the lower validation accuracy, compared to the others that are preventative of this issue (but it still can occur). Though not asked in this question, there are multiple ways to prevent the exploding gradient, with gradient clipping, ReLU activation function, and changing the RNN architecture, which are the next three RNNs used.\n",
    "\n",
    "### Long Short-term Memory (LSTM)\n",
    "LSTM earned a much better score than the standard RNN. This is because it is a Gated Recurrent Network (GRN) that alleviates some difficulties in training the RNN and deals with the vanishing gradient problem. This is done with a three-step approach that gives the NN the ability to filter the cell state and hidden state from one step to the next. Though the parameters are greatly inflated from the standard RNN, LSTM has more representational power.\n",
    "\n",
    "The gates in LSTM are; Forget gates, which control what information is forgotten from the previous cell state. The cell state erases some content from the last cell state and writes some new content. It is computed as a weighted nonlinear combination. Input gates control what part of the new cell contents are written to the cell. And is similarly computed as a weighted linear combination of the previous hidden state. Output gates control what part of the cell contents are output to the hidden state. The hidden state reads some content from the cell state that outputs some content from the cell state by elementwise multiplication of the output gate with the cell content.\n",
    "\n",
    "### Gated Recurrent Units (GRU)\n",
    "GRU achieved the second-highest validation score. Being also a GRN, GRU has some advantages over LSTM. GRU has fewer parameters by being a simpler alternative, making it faster to train and perform comparable performance. Because of this, GRU is the most widely used. GRU and LSTM are different because GRU does not maintain a separate cell state to store long-term information.\n",
    "\n",
    "### Bidirectional Long Short-term Memory \n",
    "BiLSTM achieved the highest accuracy. This is done by combining forward and backward LSTMs, meaning an information flow from the left and right sides. The two hidden states created from both LSTMs are concatenated. This gives the NN more representational power by looking into the past and future contexts by looking at both left and the right context for the current prediction. The downside to this NN is that it essentially trains two NN that requires far more computational power to train than normal LSTMs. But we are not stuck with BiLSTM; we can use other RNNs or GRNs bidirectionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versatility of LSTMs\n",
    "LSTMs are unique in that their applications are extensive and can be implemented into many different scenarios. One potential application is when considered with transportation is passenger flow. Looking into how LSTMs uses in transportation, I ran into quite a few journals talking about what is described as a Spatio-Temporal LSTM (ST-LSTM) used to extract Spatio-temporal features from the data and combines them as the input.\n",
    "\n",
    "For this journal, the researchers are wanting to solve the problem for the short-term traffic forecasts, describing the importance as \"Accurate short-term traffic forecast can provide technical support for the surveillance and the forewarning of passenger flow.\" And choose to research the ST-LSTM because with rail transits uniformity, this problem applies well because the spatial correlation can be transformed into the time cost. \n",
    "\n",
    "The input to the model is the summation of estimated passenger flows from the other stations. The output is the forecast of the exit passenger flor at station $j$ in time $t$, a passenger from station $k$ in time $t - \\Delta * T _{k,j}$ have to be considered, these being the spatial, temporal components. \n",
    "\n",
    "for the loss function, this journal used $loss = || \\hat{x} _{out, j, t} - x _{out, j, t} ||^2_2$ where $\\hat{x} _{out, j, t}$ is the forecast pf station $j$ in time $t$ and $x _{out, j, t}$ is the actual output. Outside of this reasearch paper, mean square error would also be an effective loss function. \n",
    "\n",
    "The use cases for LSTM are quite numerous as the network is well suited to classifying processing and making predictions based on time series. Because of this, the NN is very prominent in Natural Language Processing (NLP), but as shown in this journal and countless other examples, it is a very powerful NN that, when implemented efficiently, can be very successful in tasks unrelated to NLP. \n",
    "\n",
    "Citation and journal link:\n",
    "JOUR, Ramalhinho Helena, Tang Qicheng, Yang Mengning, Yang Ying, 2019, 2019/02/06, ST-LSTM: A Deep Learning Approach Combined Spatio-Temporal Features for Short-Term Forecast in Rail Transit, 8392592, 2019, The short-term forecast of rail transit is one of the most essential issues in urban intelligent transportation system (ITS). Accurate forecast result can provide support for the forewarning of flow outburst and enables passengers to make an appropriate travel plan. Therefore, it is significant to develop a more accurate forecast model. Long short-term memory (LSTM) network has been proved to be effective on data with temporal features. However, it cannot process the correlation between time and space in rail transit. As a result, a novel forecast model combining spatio-temporal features based on LSTM network (ST-LSTM) is proposed. Different from other forecast methods, ST-LSTM network uses a new method to extract spatio-temporal features from the data and combines them together as the input. Compared with other conventional models, ST-LSTM network can achieve a better performance in experiments., 0197-6729, https://doi.org/10.1155/2019/8392592, 10.1155/2019/8392592, Journal of Advanced Transportation, Hindawi https://www.hindawi.com/journals/jat/2019/8392592/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
